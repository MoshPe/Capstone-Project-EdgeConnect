{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0393c016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:47.591692Z",
     "iopub.status.busy": "2023-04-10T08:55:47.591325Z",
     "iopub.status.idle": "2023-04-10T08:55:47.599564Z",
     "shell.execute_reply": "2023-04-10T08:55:47.598619Z"
    },
    "papermill": {
     "duration": 0.017108,
     "end_time": "2023-04-10T08:55:47.601913",
     "exception": false,
     "start_time": "2023-04-10T08:55:47.584805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# precision = []\n",
    "# recall = []\n",
    "# loss_gen = []\n",
    "# loss_dis = []\n",
    "# epoch = []\n",
    "# iterations = []\n",
    "# loss_fm = []\n",
    "# psnr = []\n",
    "# with open('/kaggle/input/log-joint/log_joint.dat', 'r') as f:\n",
    "#     for line in f:\n",
    "#         splits = line.split()\n",
    "#         epoch.append(splits[0])\n",
    "#         iterations.append(splits[1])\n",
    "#         loss_dis.append(splits[2])\n",
    "#         loss_gen.append(splits[3])\n",
    "#         loss_fm.append(splits[4])\n",
    "#         precision.append(splits[5])\n",
    "#         recall.append(splits[6])\n",
    "#         psnr.append(splits[12])\n",
    "        \n",
    "\n",
    "# def generateXY(arr1, arr2):\n",
    "#     b_size = 50\n",
    "#     a = np.array(arr1).astype(np.float)\n",
    "#     bl = a.size // b_size\n",
    "#     l = a.size - b_size * bl\n",
    "#     r = b_size - l\n",
    "#     assert l * (bl + 1) + r * bl == a.size\n",
    "#     al, ar = a[:l * (bl + 1)], a[l * (bl + 1):]\n",
    "#     al = al.reshape(l, bl + 1)\n",
    "#     ar = ar.reshape(r, bl)\n",
    "#     b = np.concatenate((al.mean(axis = 1), ar.mean(axis = 1)))        \n",
    "#     y = b    \n",
    "\n",
    "#     b_size = 50\n",
    "#     a = np.array(arr2).astype(np.float)\n",
    "#     bl = a.size // b_size\n",
    "#     l = a.size - b_size * bl\n",
    "#     r = b_size - l\n",
    "#     assert l * (bl + 1) + r * bl == a.size\n",
    "#     al, ar = a[:l * (bl + 1)], a[l * (bl + 1):]\n",
    "#     al = al.reshape(l, bl + 1)\n",
    "#     ar = ar.reshape(r, bl)\n",
    "#     b = np.concatenate((al.mean(axis = 1), ar.mean(axis = 1)))        \n",
    "#     x = b\n",
    "#     return x, y\n",
    "    \n",
    "    \n",
    "# x, y = generateXY(precision, iterations)\n",
    "# plt.plot(x,y)\n",
    "# plt.xlabel('iterations')\n",
    "# plt.ylabel('precision')\n",
    "# plt.title(\"A simple line graph\")\n",
    "# plt.show()\n",
    "\n",
    "# x, y = generateXY(recall, iterations)\n",
    "# plt.plot(x,y)\n",
    "# plt.xlabel('iterations')\n",
    "# plt.ylabel('recall')\n",
    "# plt.title(\"A simple line graph\")\n",
    "# plt.show()\n",
    "\n",
    "# x, y = generateXY(loss_gen, iterations)\n",
    "# plt.plot(x,y)\n",
    "# plt.xlabel('iterations')\n",
    "# plt.ylabel('loss_gen')\n",
    "# plt.title(\"A simple line graph\")\n",
    "# plt.show()\n",
    "\n",
    "# x, y = generateXY(loss_dis, iterations)\n",
    "# plt.plot(x,y)\n",
    "# plt.xlabel('iterations')\n",
    "# plt.ylabel('loss_dis')\n",
    "# plt.title(\"A simple line graph\")\n",
    "# plt.show()\n",
    "\n",
    "# x, y = generateXY(psnr, iterations)\n",
    "# plt.plot(x,y)\n",
    "# plt.xlabel('iterations')\n",
    "# plt.ylabel('P')\n",
    "# plt.title(\"A simple line graph\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c1c63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:47.611371Z",
     "iopub.status.busy": "2023-04-10T08:55:47.611081Z",
     "iopub.status.idle": "2023-04-10T08:55:47.623568Z",
     "shell.execute_reply": "2023-04-10T08:55:47.622465Z"
    },
    "papermill": {
     "duration": 0.020265,
     "end_time": "2023-04-10T08:55:47.626280",
     "exception": false,
     "start_time": "2023-04-10T08:55:47.606015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self):\n",
    "        self._dict = dict()\n",
    "        self._dict['PATH'] = os.path.dirname('/kaggle/working/')\n",
    "    def __getattr__(self, name):\n",
    "        if self._dict.get(name) is not None:\n",
    "            return self._dict[name]\n",
    "        if DEFAULT_CONFIG.get(name) is not None:\n",
    "            return DEFAULT_CONFIG[name]\n",
    "        return None\n",
    "\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "    'MODE': 1,                      # 1: train, 2: test, 3: eval\n",
    "    'MODEL': 4,                     # 1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\n",
    "    'MASK': 1,                      # 1: random block, 2: half, 3: external, 4: (external, random block), 5: (external, random block, half)\n",
    "    'EDGE': 1,                      # 1: canny, 2: external\n",
    "    'NMS': 1,                       # 0: no non-max-suppression, 1: applies non-max-suppression on the external edges by multiplying by Canny\n",
    "    'SEED': 10,                     # random seed\n",
    "    'GPU': [0],                     # list of gpu ids\n",
    "    'DEBUG': 0,                     # turns on debugging mode\n",
    "    'VERBOSE': 2,                   # turns on verbose mode in the output console\n",
    "\n",
    "    'LR': 0.0001,                  # learning rate\n",
    "    'D2G_LR': 0.1,                  # discriminator/generator learning rate ratio\n",
    "    'BETA1': 0.0,                   # adam optimizer beta1\n",
    "    'BETA2': 0.9,                   # adam optimizer beta2\n",
    "    'BATCH_SIZE': 8,                # input batch size for training\n",
    "    'INPUT_SIZE': 256,              # input image size for training 0 for original size\n",
    "    'SIGMA': 2,                     # standard deviation of the Gaussian filter used in Canny edge detector (0: random, -1: no edge)\n",
    "    'MAX_ITERS': 2e6,               # maximum number of iterations to train the model\n",
    "\n",
    "    'EDGE_THRESHOLD': 0.5,          # edge detection threshold\n",
    "    'L1_LOSS_WEIGHT': 1,            # l1 loss weight\n",
    "    'FM_LOSS_WEIGHT': 10,           # feature-matching loss weight\n",
    "    'STYLE_LOSS_WEIGHT': 250,         # style loss weight\n",
    "    'CONTENT_LOSS_WEIGHT': 0.1,       # perceptual loss weight\n",
    "    'INPAINT_ADV_LOSS_WEIGHT': 0.1,# adversarial loss weight\n",
    "\n",
    "    'GAN_LOSS': 'nsgan',            # nsgan | lsgan | hinge\n",
    "    'GAN_POOL_SIZE': 0,             # fake images pool size\n",
    "\n",
    "    'SAVE_INTERVAL': 1000,          # how many iterations to wait before saving model (0: never)\n",
    "    'SAMPLE_INTERVAL': 1000,        # how many iterations to wait before sampling (0: never)\n",
    "    'SAMPLE_SIZE': 12,              # number of images to sample\n",
    "    'EVAL_INTERVAL': 0,             # how many iterations to wait before model evaluation (0: never)\n",
    "    'LOG_INTERVAL': 10,             # how many iterations to wait before logging training status (0: never)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da61bc3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:47.635624Z",
     "iopub.status.busy": "2023-04-10T08:55:47.635371Z",
     "iopub.status.idle": "2023-04-10T08:55:51.199393Z",
     "shell.execute_reply": "2023-04-10T08:55:51.198370Z"
    },
    "papermill": {
     "duration": 3.571624,
     "end_time": "2023-04-10T08:55:51.201943",
     "exception": false,
     "start_time": "2023-04-10T08:55:47.630319",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import scipy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import canny\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,input_size, filepath, augment=True, training=True, isVal=False):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.augment = augment\n",
    "        self.training = training\n",
    "        self.isVal = isVal\n",
    "        if isVal == False:\n",
    "            with open('/kaggle/input/file-list/places365_train_standard.txt', 'r') as f:\n",
    "                self.imageNames = [line.split(None, 1)[0] for line in f]\n",
    "                self.imageNames = self.imageNames[::72]\n",
    "                print(len(self.imageNames))\n",
    "        else:\n",
    "             with open('/kaggle/input/file-list/places365_val.txt', 'r') as f:\n",
    "                self.imageNames = [line.split(None, 1)[0] for line in f]\n",
    "        \n",
    "        self.data = self.load_flist(filepath)\n",
    "        self.input_size = input_size\n",
    "#             self.edge_data = self.load_flist(edge_flist)\n",
    "#             self.mask_data = self.load_flist(mask_flist)\n",
    "\n",
    "        self.nms = config.NMS\n",
    "        self.sigma = config.SIGMA\n",
    "        self.edge = config.EDGE\n",
    "        self.mask = config.MASK\n",
    "\n",
    "        # in test mode, there's a one-to-one relationship between mask and image\n",
    "        # masks are loaded non random\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            item = self.load_item(index)\n",
    "        except:\n",
    "            print('loading error: ' + self.data[index])\n",
    "            item = self.load_item(0)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def load_flist(self, flist):\n",
    "        imagesPath = []\n",
    "        for filename in self.imageNames:\n",
    "            if self.isVal == False:\n",
    "                imagesPath.append(os.path.join(flist,filename[1:]))\n",
    "            else:\n",
    "                imagesPath.append(os.path.join(flist,filename))\n",
    "            \n",
    "#         To run on subset of images:\n",
    "#         if self.isVal == False:\n",
    "#             imagesPath = imagesPath[304896:]\n",
    "        return imagesPath\n",
    "    def load_name(self, index):\n",
    "        name = self.data[index]\n",
    "        return os.path.basename(name)\n",
    "    \n",
    "    def load_item(self, index):\n",
    "        size = self.input_size\n",
    "        img =  cv2.imread(self.data[index])\n",
    "        if len(img.shape) < 3:\n",
    "            img = gray2rgb(img)\n",
    "        if size != 0:\n",
    "            img = self.resize(img, size, size)\n",
    "        img_gray = rgb2gray(img)\n",
    "        mask = self.load_mask(img, index)\n",
    "        edge = self.load_edge(img_gray, index, mask)\n",
    "        if self.augment and np.random.binomial(1, 0.5) > 0:\n",
    "            img = img[:, ::-1, ...]\n",
    "            img_gray = img_gray[:, ::-1, ...]\n",
    "            edge = edge[:, ::-1, ...]\n",
    "            mask = mask[:, ::-1, ...]\n",
    "\n",
    "        return self.to_tensor(img), self.to_tensor(img_gray), self.to_tensor(edge), self.to_tensor(mask)\n",
    "    \n",
    "    def load_edge(self, img, index, mask):\n",
    "        sigma = self.sigma\n",
    "        mask = None if self.training else (1 - mask / 255).astype(np.bool)\n",
    "\n",
    "        # canny\n",
    "        # no edge\n",
    "        if sigma == -1:\n",
    "            return np.zeros(img.shape).astype(np.float)\n",
    "\n",
    "        # random sigma\n",
    "        if sigma == 0:\n",
    "            sigma = random.randint(1, 4)\n",
    "\n",
    "        return canny(img, sigma=sigma, mask=mask).astype(np.float)\n",
    "    \n",
    "    def load_mask(self, img, index):\n",
    "        imgh, imgw = img.shape[0:2]\n",
    "        mask_type = self.mask\n",
    "\n",
    "        # external + random block\n",
    "        if mask_type == 4:\n",
    "            mask_type = 1 if np.random.binomial(1, 0.5) == 1 else 3\n",
    "\n",
    "        # external + random block + half\n",
    "        elif mask_type == 5:\n",
    "            mask_type = np.random.randint(1, 4)\n",
    "\n",
    "        # random block\n",
    "        if mask_type == 1:\n",
    "            return self.create_mask(imgw, imgh, imgw // 8, imgh // 8)\n",
    "\n",
    "        # half\n",
    "        if mask_type == 2:\n",
    "            # randomly choose right or left\n",
    "            return self.create_mask(imgw, imgh, imgw // 2, imgh, 0 if random.random() < 0.5 else imgw // 2, 0)\n",
    "\n",
    "        # external\n",
    "        if mask_type == 3:\n",
    "            mask_index = random.randint(0, len(self.mask_data) - 1)\n",
    "            mask = imread(self.mask_data[mask_index])\n",
    "            mask = self.resize(mask, imgh, imgw)\n",
    "            mask = (mask > 0).astype(np.uint8) * 255       # threshold due to interpolation\n",
    "            return mask\n",
    "\n",
    "        # test mode: load mask non random\n",
    "        if mask_type == 6:\n",
    "            mask = imread(self.mask_data[index])\n",
    "            mask = self.resize(mask, imgh, imgw, centerCrop=False)\n",
    "            mask = rgb2gray(mask)\n",
    "            mask = (mask > 0).astype(np.uint8) * 255\n",
    "            return mask\n",
    "        \n",
    "    def resize(self, img, height, width, centerCrop=True):\n",
    "        imgh, imgw = img.shape[0:2]\n",
    "        if centerCrop and imgh != imgw:\n",
    "            # center crop\n",
    "            side = np.minimum(imgh, imgw)\n",
    "            j = (imgh - side) // 2\n",
    "            i = (imgw - side) // 2\n",
    "            img = img[j:j + side, i:i + side, ...]\n",
    "\n",
    "        img = cv2.resize(img, (height, width))\n",
    "        return img  \n",
    "    \n",
    "    def to_tensor(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "#         plt.imshow(img)    \n",
    "#         plt.show()\n",
    "        img_t = F.to_tensor(img).float()\n",
    "        return img_t\n",
    "    \n",
    "    def create_mask(self, width, height, mask_width, mask_height, x=None, y=None):\n",
    "        mask = np.zeros((height, width))\n",
    "        mask_x = x if x is not None else random.randint(0, width - mask_width)\n",
    "        mask_y = y if y is not None else random.randint(0, height - mask_height)\n",
    "        mask[mask_y:mask_y + mask_height, mask_x:mask_x + mask_width] = 1\n",
    "        return mask\n",
    "    def create_iterator(self, batch_size):\n",
    "        while True:\n",
    "            sample_loader = DataLoader(\n",
    "                dataset=self,\n",
    "                batch_size=batch_size,\n",
    "                drop_last=True\n",
    "            )\n",
    "\n",
    "            for item in sample_loader:\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966e50e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:51.212029Z",
     "iopub.status.busy": "2023-04-10T08:55:51.211574Z",
     "iopub.status.idle": "2023-04-10T08:55:51.244916Z",
     "shell.execute_reply": "2023-04-10T08:55:51.243818Z"
    },
    "papermill": {
     "duration": 0.041185,
     "end_time": "2023-04-10T08:55:51.247370",
     "exception": false,
     "start_time": "2023-04-10T08:55:51.206185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNetwork, self).__init__()\n",
    "\n",
    "    def init_weights(self, init_type='normal', gain=0.02):\n",
    "        '''\n",
    "        initialize network's weights\n",
    "        init_type: normal | xavier | kaiming | orthogonal\n",
    "        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n",
    "        '''\n",
    "\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "            elif classname.find('BatchNorm2d') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "\n",
    "class InpaintGenerator(BaseNetwork):\n",
    "    def __init__(self, residual_blocks=8, init_weights=True):\n",
    "        super(InpaintGenerator, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=4, out_channels=64, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256, track_running_stats=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        blocks = []\n",
    "        for _ in range(residual_blocks):\n",
    "            block = ResnetBlock(256, 2)\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.middle = nn.Sequential(*blocks)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, padding=0),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.decoder(x)\n",
    "        x = (torch.tanh(x) + 1) / 2\n",
    "        return x\n",
    "    \n",
    "class EdgeGenerator(BaseNetwork):\n",
    "    def __init__(self, residual_blocks=8, use_spectral_norm=True, init_weights=True):\n",
    "        super(EdgeGenerator, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            spectral_norm(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, padding=0), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(256, track_running_stats=False),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        blocks = []\n",
    "        for _ in range(residual_blocks):\n",
    "            block = ResnetBlock(256, 2, use_spectral_norm=use_spectral_norm)\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.middle = nn.Sequential(*blocks)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            spectral_norm(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(128, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            spectral_norm(nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(64, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels=64, out_channels=1, kernel_size=7, padding=0),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.decoder(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(BaseNetwork):\n",
    "    def __init__(self, in_channels, use_sigmoid=True, use_spectral_norm=True, init_weights=True):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "\n",
    "        self.conv1 = self.features = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "        )\n",
    "\n",
    "        if init_weights:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        outputs = conv5\n",
    "        if self.use_sigmoid:\n",
    "            outputs = torch.sigmoid(conv5)\n",
    "\n",
    "        return outputs, [conv1, conv2, conv3, conv4, conv5]\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dilation=1, use_spectral_norm=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(dilation),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=dilation, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            spectral_norm(nn.Conv2d(in_channels=dim, out_channels=dim, kernel_size=3, padding=0, dilation=1, bias=not use_spectral_norm), use_spectral_norm),\n",
    "            nn.InstanceNorm2d(dim, track_running_stats=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "\n",
    "        # Remove ReLU at the end of the residual block\n",
    "        # http://torch.ch/blog/2016/02/04/resnets.html\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def spectral_norm(module, mode=True):\n",
    "    if mode:\n",
    "        return nn.utils.spectral_norm(module)\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7105724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:51.256606Z",
     "iopub.status.busy": "2023-04-10T08:55:51.256336Z",
     "iopub.status.idle": "2023-04-10T08:55:51.287632Z",
     "shell.execute_reply": "2023-04-10T08:55:51.286670Z"
    },
    "papermill": {
     "duration": 0.038508,
     "end_time": "2023-04-10T08:55:51.289747",
     "exception": false,
     "start_time": "2023-04-10T08:55:51.251239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class AdversarialLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "    Adversarial loss\n",
    "    https://arxiv.org/abs/1711.10337\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, type='nsgan', target_real_label=1.0, target_fake_label=0.0):\n",
    "        r\"\"\"\n",
    "        type = nsgan | lsgan | hinge\n",
    "        \"\"\"\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "\n",
    "        self.type = type\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "\n",
    "        if type == 'nsgan':\n",
    "            self.criterion = nn.BCELoss()\n",
    "\n",
    "        elif type == 'lsgan':\n",
    "            self.criterion = nn.MSELoss()\n",
    "\n",
    "        elif type == 'hinge':\n",
    "            self.criterion = nn.ReLU()\n",
    "\n",
    "    def __call__(self, outputs, is_real, is_disc=None):\n",
    "        if self.type == 'hinge':\n",
    "            if is_disc:\n",
    "                if is_real:\n",
    "                    outputs = -outputs\n",
    "                return self.criterion(1 + outputs).mean()\n",
    "            else:\n",
    "                return (-outputs).mean()\n",
    "\n",
    "        else:\n",
    "            labels = (self.real_label if is_real else self.fake_label).expand_as(outputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            return loss\n",
    "\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "    Perceptual loss, VGG-based\n",
    "    https://arxiv.org/abs/1603.08155\n",
    "    https://github.com/dxyang/StyleTransfer/blob/master/utils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.add_module('vgg', VGG19())\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "    def compute_gram(self, x):\n",
    "        b, ch, h, w = x.size()\n",
    "        f = x.view(b, ch, w * h)\n",
    "        f_T = f.transpose(1, 2)\n",
    "        G = f.bmm(f_T) / (h * w * ch)\n",
    "\n",
    "        return G\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        # Compute features\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "\n",
    "        # Compute loss\n",
    "        style_loss = 0.0\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu2_2']), self.compute_gram(y_vgg['relu2_2']))\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu3_4']), self.compute_gram(y_vgg['relu3_4']))\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu4_4']), self.compute_gram(y_vgg['relu4_4']))\n",
    "        style_loss += self.criterion(self.compute_gram(x_vgg['relu5_2']), self.compute_gram(y_vgg['relu5_2']))\n",
    "\n",
    "        return style_loss\n",
    "\n",
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "    Perceptual loss, VGG-based\n",
    "    https://arxiv.org/abs/1603.08155\n",
    "    https://github.com/dxyang/StyleTransfer/blob/master/utils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights=[1.0, 1.0, 1.0, 1.0, 1.0]):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.add_module('vgg', VGG19())\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        # Compute features\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "\n",
    "        content_loss = 0.0\n",
    "        content_loss += self.weights[0] * self.criterion(x_vgg['relu1_1'], y_vgg['relu1_1'])\n",
    "        content_loss += self.weights[1] * self.criterion(x_vgg['relu2_1'], y_vgg['relu2_1'])\n",
    "        content_loss += self.weights[2] * self.criterion(x_vgg['relu3_1'], y_vgg['relu3_1'])\n",
    "        content_loss += self.weights[3] * self.criterion(x_vgg['relu4_1'], y_vgg['relu4_1'])\n",
    "        content_loss += self.weights[4] * self.criterion(x_vgg['relu5_1'], y_vgg['relu5_1'])\n",
    "\n",
    "\n",
    "        return content_loss\n",
    "\n",
    "\n",
    "\n",
    "class VGG19(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        features = models.vgg19(pretrained=True).features\n",
    "        self.relu1_1 = torch.nn.Sequential()\n",
    "        self.relu1_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu2_1 = torch.nn.Sequential()\n",
    "        self.relu2_2 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu3_1 = torch.nn.Sequential()\n",
    "        self.relu3_2 = torch.nn.Sequential()\n",
    "        self.relu3_3 = torch.nn.Sequential()\n",
    "        self.relu3_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu4_1 = torch.nn.Sequential()\n",
    "        self.relu4_2 = torch.nn.Sequential()\n",
    "        self.relu4_3 = torch.nn.Sequential()\n",
    "        self.relu4_4 = torch.nn.Sequential()\n",
    "\n",
    "        self.relu5_1 = torch.nn.Sequential()\n",
    "        self.relu5_2 = torch.nn.Sequential()\n",
    "        self.relu5_3 = torch.nn.Sequential()\n",
    "        self.relu5_4 = torch.nn.Sequential()\n",
    "\n",
    "        for x in range(2):\n",
    "            self.relu1_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(2, 4):\n",
    "            self.relu1_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(4, 7):\n",
    "            self.relu2_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(7, 9):\n",
    "            self.relu2_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(9, 12):\n",
    "            self.relu3_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(12, 14):\n",
    "            self.relu3_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(14, 16):\n",
    "            self.relu3_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(16, 18):\n",
    "            self.relu3_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(18, 21):\n",
    "            self.relu4_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(21, 23):\n",
    "            self.relu4_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(23, 25):\n",
    "            self.relu4_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(25, 27):\n",
    "            self.relu4_4.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(27, 30):\n",
    "            self.relu5_1.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(30, 32):\n",
    "            self.relu5_2.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(32, 34):\n",
    "            self.relu5_3.add_module(str(x), features[x])\n",
    "\n",
    "        for x in range(34, 36):\n",
    "            self.relu5_4.add_module(str(x), features[x])\n",
    "\n",
    "        # don't need the gradients, just want the features\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        relu1_1 = self.relu1_1(x)\n",
    "        relu1_2 = self.relu1_2(relu1_1)\n",
    "\n",
    "        relu2_1 = self.relu2_1(relu1_2)\n",
    "        relu2_2 = self.relu2_2(relu2_1)\n",
    "\n",
    "        relu3_1 = self.relu3_1(relu2_2)\n",
    "        relu3_2 = self.relu3_2(relu3_1)\n",
    "        relu3_3 = self.relu3_3(relu3_2)\n",
    "        relu3_4 = self.relu3_4(relu3_3)\n",
    "\n",
    "        relu4_1 = self.relu4_1(relu3_4)\n",
    "        relu4_2 = self.relu4_2(relu4_1)\n",
    "        relu4_3 = self.relu4_3(relu4_2)\n",
    "        relu4_4 = self.relu4_4(relu4_3)\n",
    "\n",
    "        relu5_1 = self.relu5_1(relu4_4)\n",
    "        relu5_2 = self.relu5_2(relu5_1)\n",
    "        relu5_3 = self.relu5_3(relu5_2)\n",
    "        relu5_4 = self.relu5_4(relu5_3)\n",
    "\n",
    "        out = {\n",
    "            'relu1_1': relu1_1,\n",
    "            'relu1_2': relu1_2,\n",
    "\n",
    "            'relu2_1': relu2_1,\n",
    "            'relu2_2': relu2_2,\n",
    "\n",
    "            'relu3_1': relu3_1,\n",
    "            'relu3_2': relu3_2,\n",
    "            'relu3_3': relu3_3,\n",
    "            'relu3_4': relu3_4,\n",
    "\n",
    "            'relu4_1': relu4_1,\n",
    "            'relu4_2': relu4_2,\n",
    "            'relu4_3': relu4_3,\n",
    "            'relu4_4': relu4_4,\n",
    "\n",
    "            'relu5_1': relu5_1,\n",
    "            'relu5_2': relu5_2,\n",
    "            'relu5_3': relu5_3,\n",
    "            'relu5_4': relu5_4,\n",
    "        }\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8bbb0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:51.299557Z",
     "iopub.status.busy": "2023-04-10T08:55:51.298743Z",
     "iopub.status.idle": "2023-04-10T08:55:51.333374Z",
     "shell.execute_reply": "2023-04-10T08:55:51.332511Z"
    },
    "papermill": {
     "duration": 0.042048,
     "end_time": "2023-04-10T08:55:51.335600",
     "exception": false,
     "start_time": "2023-04-10T08:55:51.293552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, name, config):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "        self.name = name\n",
    "        self.config = config\n",
    "        self.iteration = 0\n",
    "\n",
    "#         print(config.PATH)\n",
    "        \n",
    "        self.gen_weights_path = os.path.join(config.PATH, name + '_gen.pth')\n",
    "        self.dis_weights_path = os.path.join(config.PATH, name + '_dis.pth')\n",
    "\n",
    "    def load(self):\n",
    "#         print('in load', self.name, self.gen_weights_path)\n",
    "        gen_weights_path = '/kaggle/input/trained-data/' + self.name + '_gen.pth'\n",
    "        dis_weights_path = '/kaggle/input/trained-data/' + self.name + '_dis.pth'\n",
    "        if os.path.exists(gen_weights_path):\n",
    "            print('Loading %s generator...' % self.name)\n",
    "            if torch.cuda.is_available():\n",
    "                data = torch.load(gen_weights_path)\n",
    "            else:\n",
    "                data = torch.load(gen_weights_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "            self.generator.load_state_dict(data['generator'])\n",
    "            self.iteration = data['iteration']\n",
    "\n",
    "        # load discriminator only when training\n",
    "        if self.config.MODE == 1 and os.path.exists(dis_weights_path):\n",
    "            print('Loading %s discriminator...' % self.name)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                data = torch.load(dis_weights_path)\n",
    "            else:\n",
    "                data = torch.load(dis_weights_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "            self.discriminator.load_state_dict(data['discriminator'])\n",
    "\n",
    "    def save(self):\n",
    "        print('\\nsaving %s...\\n' % self.name)\n",
    "        torch.save({\n",
    "            'iteration': self.iteration,\n",
    "            'generator': self.generator.state_dict()\n",
    "        }, self.gen_weights_path)\n",
    "\n",
    "        torch.save({\n",
    "            'discriminator': self.discriminator.state_dict()\n",
    "        }, self.dis_weights_path)\n",
    "\n",
    "\n",
    "class EdgeModel(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super(EdgeModel, self).__init__('EdgeModel', config)\n",
    "\n",
    "        # generator input: [grayscale(1) + edge(1) + mask(1)]\n",
    "        # discriminator input: (grayscale(1) + edge(1))\n",
    "        generator = EdgeGenerator(use_spectral_norm=True)\n",
    "        discriminator = Discriminator(in_channels=2, use_sigmoid=config.GAN_LOSS != 'hinge')\n",
    "        if len(config.GPU) > 1:\n",
    "            generator = nn.DataParallel(generator, device_ids=[0, 1])\n",
    "            discriminator = nn.DataParallel(discriminator, config.GPU)\n",
    "        l1_loss = nn.L1Loss()\n",
    "        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n",
    "\n",
    "        self.add_module('generator', generator)\n",
    "        self.add_module('discriminator', discriminator)\n",
    "\n",
    "        self.add_module('l1_loss', l1_loss)\n",
    "        self.add_module('adversarial_loss', adversarial_loss)\n",
    "\n",
    "        self.gen_optimizer = optim.Adam(\n",
    "            params=generator.parameters(),\n",
    "            lr=float(config.LR),\n",
    "            betas=(config.BETA1, config.BETA2)\n",
    "        )\n",
    "\n",
    "        self.dis_optimizer = optim.Adam(\n",
    "            params=discriminator.parameters(),\n",
    "            lr=float(config.LR) * float(config.D2G_LR),\n",
    "            betas=(config.BETA1, config.BETA2)\n",
    "        )\n",
    "\n",
    "    def process(self, images, edges, masks):\n",
    "        self.iteration += 1\n",
    "\n",
    "\n",
    "        # zero optimizers\n",
    "        self.gen_optimizer.zero_grad()\n",
    "        self.dis_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # process outputs\n",
    "        outputs = self(images, edges, masks)\n",
    "        gen_loss = 0\n",
    "        dis_loss = 0\n",
    "\n",
    "\n",
    "        # discriminator loss\n",
    "        dis_input_real = torch.cat((images, edges), dim=1)\n",
    "        dis_input_fake = torch.cat((images, outputs.detach()), dim=1)\n",
    "        dis_real, dis_real_feat = self.discriminator(dis_input_real)        # in: (grayscale(1) + edge(1))\n",
    "        dis_fake, dis_fake_feat = self.discriminator(dis_input_fake)        # in: (grayscale(1) + edge(1))\n",
    "        dis_real_loss = self.adversarial_loss(dis_real, True, True)\n",
    "        dis_fake_loss = self.adversarial_loss(dis_fake, False, True)\n",
    "        dis_loss += (dis_real_loss.clone() + dis_fake_loss.clone()) / 2\n",
    "\n",
    "\n",
    "        # generator adversarial loss\n",
    "        gen_input_fake = torch.cat((images, outputs), dim=1)\n",
    "        gen_fake, gen_fake_feat = self.discriminator(gen_input_fake)        # in: (grayscale(1) + edge(1))\n",
    "        gen_gan_loss = self.adversarial_loss(gen_fake, True, False)\n",
    "        gen_loss += gen_gan_loss.clone()\n",
    "\n",
    "\n",
    "        # generator feature matching loss\n",
    "        gen_fm_loss = 0\n",
    "        for i in range(len(dis_real_feat)):\n",
    "            gen_fm_loss += self.l1_loss(gen_fake_feat[i], dis_real_feat[i].detach())\n",
    "        gen_fm_loss = gen_fm_loss * self.config.FM_LOSS_WEIGHT\n",
    "        gen_loss += gen_fm_loss\n",
    "\n",
    "\n",
    "        # create logs\n",
    "        logs = [\n",
    "            (\"l_d1\", dis_loss.item()),\n",
    "            (\"l_g1\", gen_gan_loss.item()),\n",
    "            (\"l_fm\", gen_fm_loss.item()),\n",
    "        ]\n",
    "\n",
    "        return outputs, gen_loss, dis_loss, logs\n",
    "\n",
    "    def forward(self, images, edges, masks):\n",
    "        edges_masked = (edges * (1 - masks))\n",
    "        images_masked = (images * (1 - masks)) + masks\n",
    "        inputs = torch.cat((images_masked, edges_masked, masks), dim=1)\n",
    "        outputs = self.generator(inputs)                                    # in: [grayscale(1) + edge(1) + mask(1)]\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, gen_loss=None, dis_loss=None):\n",
    "        if dis_loss is not None:\n",
    "            dis_loss.backward()\n",
    "\n",
    "        if gen_loss is not None:\n",
    "            gen_loss.backward()\n",
    "        self.gen_optimizer.step()\n",
    "        self.dis_optimizer.step()\n",
    "        \n",
    "class InpaintingModel(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super(InpaintingModel, self).__init__('InpaintingModel', config)\n",
    "\n",
    "        # generator input: [rgb(3) + edge(1)]\n",
    "        # discriminator input: [rgb(3)]\n",
    "        generator = InpaintGenerator()\n",
    "        discriminator = Discriminator(in_channels=3, use_sigmoid=config.GAN_LOSS != 'hinge')\n",
    "        if len(config.GPU) > 1:\n",
    "            generator = nn.DataParallel(generator, config.GPU)\n",
    "            discriminator = nn.DataParallel(discriminator , config.GPU)\n",
    "\n",
    "        l1_loss = nn.L1Loss()\n",
    "        perceptual_loss = PerceptualLoss()\n",
    "        style_loss = StyleLoss()\n",
    "        adversarial_loss = AdversarialLoss(type=config.GAN_LOSS)\n",
    "\n",
    "        self.add_module('generator', generator)\n",
    "        self.add_module('discriminator', discriminator)\n",
    "\n",
    "        self.add_module('l1_loss', l1_loss)\n",
    "        self.add_module('perceptual_loss', perceptual_loss)\n",
    "        self.add_module('style_loss', style_loss)\n",
    "        self.add_module('adversarial_loss', adversarial_loss)\n",
    "\n",
    "        self.gen_optimizer = optim.Adam(\n",
    "            params=generator.parameters(),\n",
    "            lr=float(config.LR),\n",
    "            betas=(config.BETA1, config.BETA2)\n",
    "        )\n",
    "\n",
    "        self.dis_optimizer = optim.Adam(\n",
    "            params=discriminator.parameters(),\n",
    "            lr=float(config.LR) * float(config.D2G_LR),\n",
    "            betas=(config.BETA1, config.BETA2)\n",
    "        )\n",
    "\n",
    "    def process(self, images, edges, masks):\n",
    "        self.iteration += 1\n",
    "\n",
    "        # zero optimizers\n",
    "        self.gen_optimizer.zero_grad()\n",
    "        self.dis_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # process outputs\n",
    "        outputs = self(images, edges, masks)\n",
    "        gen_loss = 0\n",
    "        dis_loss = 0\n",
    "\n",
    "\n",
    "        # discriminator loss\n",
    "        dis_input_real = images\n",
    "        dis_input_fake = outputs.detach()\n",
    "        dis_real, _ = self.discriminator(dis_input_real)                    # in: [rgb(3)]\n",
    "        dis_fake, _ = self.discriminator(dis_input_fake)                    # in: [rgb(3)]\n",
    "        dis_real_loss = self.adversarial_loss(dis_real, True, True)\n",
    "        dis_fake_loss = self.adversarial_loss(dis_fake, False, True)\n",
    "        dis_loss += (dis_real_loss + dis_fake_loss) / 2\n",
    "\n",
    "\n",
    "        # generator adversarial loss\n",
    "        gen_input_fake = outputs\n",
    "        gen_fake, _ = self.discriminator(gen_input_fake)                    # in: [rgb(3)]\n",
    "        gen_gan_loss = self.adversarial_loss(gen_fake, True, False) * self.config.INPAINT_ADV_LOSS_WEIGHT\n",
    "        gen_loss += gen_gan_loss\n",
    "\n",
    "\n",
    "        # generator l1 loss\n",
    "        gen_l1_loss = self.l1_loss(outputs, images) * self.config.L1_LOSS_WEIGHT / torch.mean(masks)\n",
    "        gen_loss += gen_l1_loss\n",
    "\n",
    "\n",
    "        # generator perceptual loss\n",
    "        gen_content_loss = self.perceptual_loss(outputs, images)\n",
    "        gen_content_loss = gen_content_loss * self.config.CONTENT_LOSS_WEIGHT\n",
    "        gen_loss += gen_content_loss\n",
    "\n",
    "\n",
    "        # generator style loss\n",
    "        gen_style_loss = self.style_loss(outputs * masks, images * masks)\n",
    "        gen_style_loss = gen_style_loss * self.config.STYLE_LOSS_WEIGHT\n",
    "        gen_loss += gen_style_loss\n",
    "\n",
    "\n",
    "        # create logs\n",
    "        logs = [\n",
    "            (\"l_d2\", dis_loss.item()),\n",
    "            (\"l_g2\", gen_gan_loss.item()),\n",
    "            (\"l_l1\", gen_l1_loss.item()),\n",
    "            (\"l_per\", gen_content_loss.item()),\n",
    "            (\"l_sty\", gen_style_loss.item()),\n",
    "        ]\n",
    "\n",
    "        return outputs, gen_loss, dis_loss, logs\n",
    "\n",
    "    def forward(self, images, edges, masks):\n",
    "        images_masked = (images * (1 - masks).float()) + masks\n",
    "        inputs = torch.cat((images_masked, edges), dim=1)\n",
    "        outputs = self.generator(inputs)                                    # in: [rgb(3) + edge(1)]\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, gen_loss=None, dis_loss=None):\n",
    "        dis_loss.backward()\n",
    "        gen_loss.backward()\n",
    "        \n",
    "        self.dis_optimizer.step()\n",
    "        self.gen_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657ddff3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:51.344590Z",
     "iopub.status.busy": "2023-04-10T08:55:51.344318Z",
     "iopub.status.idle": "2023-04-10T08:55:51.354505Z",
     "shell.execute_reply": "2023-04-10T08:55:51.353456Z"
    },
    "papermill": {
     "duration": 0.017072,
     "end_time": "2023-04-10T08:55:51.356627",
     "exception": false,
     "start_time": "2023-04-10T08:55:51.339555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EdgeAccuracy(nn.Module):\n",
    "    \"\"\"\n",
    "    Measures the accuracy of the edge map\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super(EdgeAccuracy, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, inputs, outputs):\n",
    "        labels = (inputs > self.threshold)\n",
    "        outputs = (outputs > self.threshold)\n",
    "\n",
    "        relevant = torch.sum(labels.float())\n",
    "        selected = torch.sum(outputs.float())\n",
    "\n",
    "        if relevant == 0 and selected == 0:\n",
    "            return torch.tensor(1), torch.tensor(1)\n",
    "\n",
    "        true_positive = ((outputs == labels) * labels).float()\n",
    "        recall = torch.sum(true_positive) / (relevant + 1e-8)\n",
    "        precision = torch.sum(true_positive) / (selected + 1e-8)\n",
    "\n",
    "        return precision, recall\n",
    "\n",
    "\n",
    "class PSNR(nn.Module):\n",
    "    def __init__(self, max_val):\n",
    "        super(PSNR, self).__init__()\n",
    "\n",
    "        base10 = torch.log(torch.tensor(10.0))\n",
    "        max_val = torch.tensor(max_val).float()\n",
    "\n",
    "        self.register_buffer('base10', base10)\n",
    "        self.register_buffer('max_val', 20 * torch.log(max_val) / base10)\n",
    "\n",
    "    def __call__(self, a, b):\n",
    "        mse = torch.mean((a.float() - b.float()) ** 2)\n",
    "\n",
    "        if mse == 0:\n",
    "            return torch.tensor(0)\n",
    "\n",
    "        return self.max_val - 10 * torch.log(mse) / self.base10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4834243",
   "metadata": {
    "papermill": {
     "duration": 0.003681,
     "end_time": "2023-04-10T08:55:51.364261",
     "exception": false,
     "start_time": "2023-04-10T08:55:51.360580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba40f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:51.373417Z",
     "iopub.status.busy": "2023-04-10T08:55:51.373160Z",
     "iopub.status.idle": "2023-04-10T08:55:51.403685Z",
     "shell.execute_reply": "2023-04-10T08:55:51.402800Z"
    },
    "papermill": {
     "duration": 0.037789,
     "end_time": "2023-04-10T08:55:51.405871",
     "exception": false,
     "start_time": "2023-04-10T08:55:51.368082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "def create_mask(width, height, mask_width, mask_height, x=None, y=None):\n",
    "    mask = np.zeros((height, width))\n",
    "    mask_x = x if x is not None else random.randint(0, width - mask_width)\n",
    "    mask_y = y if y is not None else random.randint(0, height - mask_height)\n",
    "    mask[mask_y:mask_y + mask_height, mask_x:mask_x + mask_width] = 1\n",
    "    return mask\n",
    "\n",
    "def stitch_images(inputs, *outputs, img_per_row=2):\n",
    "    gap = 5\n",
    "    columns = len(outputs) + 1\n",
    "\n",
    "    width, height = inputs[0][:, :, 0].shape\n",
    "    img = Image.new('RGB', (width * img_per_row * columns + gap * (img_per_row - 1), height * int(len(inputs) / img_per_row)))\n",
    "    images = [inputs, *outputs]\n",
    "\n",
    "    for ix in range(len(inputs)):\n",
    "        xoffset = int(ix % img_per_row) * width * columns + int(ix % img_per_row) * gap\n",
    "        yoffset = int(ix / img_per_row) * height\n",
    "\n",
    "        for cat in range(len(images)):\n",
    "            im = np.array((images[cat][ix]).cpu()).astype(np.uint8).squeeze()\n",
    "            im = Image.fromarray(im)\n",
    "            img.paste(im, (xoffset + cat * width, yoffset))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def imshow(img, title=''):\n",
    "    fig = plt.gcf()\n",
    "    fig.canvas.set_window_title(title)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, interpolation='none')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def imsave(img, path):\n",
    "    im = Image.fromarray(img.cpu().numpy().astype(np.uint8).squeeze())\n",
    "    im.save(path)\n",
    "\n",
    "class Progbar(object):\n",
    "    \"\"\"Displays a progress bar.\n",
    "    Arguments:\n",
    "        target: Total number of steps expected, None if unknown.\n",
    "        width: Progress bar width on screen.\n",
    "        verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n",
    "        stateful_metrics: Iterable of string names of metrics that\n",
    "            should *not* be averaged over time. Metrics in this list\n",
    "            will be displayed as-is. All others will be averaged\n",
    "            by the progbar before display.\n",
    "        interval: Minimum visual progress update interval (in seconds).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, width=25, verbose=1, interval=0.05,\n",
    "                 stateful_metrics=None):\n",
    "        self.target = target\n",
    "        self.width = width\n",
    "        self.verbose = verbose\n",
    "        self.interval = interval\n",
    "        if stateful_metrics:\n",
    "            self.stateful_metrics = set(stateful_metrics)\n",
    "        else:\n",
    "            self.stateful_metrics = set()\n",
    "\n",
    "        self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\n",
    "                                  sys.stdout.isatty()) or\n",
    "                                 'ipykernel' in sys.modules or\n",
    "                                 'posix' in sys.modules)\n",
    "        self._total_width = 0\n",
    "        self._seen_so_far = 0\n",
    "        # We use a dict + list to avoid garbage collection\n",
    "        # issues found in OrderedDict\n",
    "        self._values = {}\n",
    "        self._values_order = []\n",
    "        self._start = time.time()\n",
    "        self._last_update = 0\n",
    "\n",
    "    def update(self, current, values=None):\n",
    "        \"\"\"Updates the progress bar.\n",
    "        Arguments:\n",
    "            current: Index of current step.\n",
    "            values: List of tuples:\n",
    "                `(name, value_for_last_step)`.\n",
    "                If `name` is in `stateful_metrics`,\n",
    "                `value_for_last_step` will be displayed as-is.\n",
    "                Else, an average of the metric over time will be displayed.\n",
    "        \"\"\"\n",
    "        values = values or []\n",
    "        for k, v in values:\n",
    "            if k not in self._values_order:\n",
    "                self._values_order.append(k)\n",
    "            if k not in self.stateful_metrics:\n",
    "                if k not in self._values:\n",
    "                    self._values[k] = [v * (current - self._seen_so_far),\n",
    "                                       current - self._seen_so_far]\n",
    "                else:\n",
    "                    self._values[k][0] += v * (current - self._seen_so_far)\n",
    "                    self._values[k][1] += (current - self._seen_so_far)\n",
    "            else:\n",
    "                self._values[k] = v\n",
    "        self._seen_so_far = current\n",
    "\n",
    "        now = time.time()\n",
    "        info = ' - %.0fs' % (now - self._start)\n",
    "        if self.verbose == 1:\n",
    "            if (now - self._last_update < self.interval and\n",
    "                    self.target is not None and current < self.target):\n",
    "                return\n",
    "\n",
    "            prev_total_width = self._total_width\n",
    "            if self._dynamic_display:\n",
    "                sys.stdout.write('\\b' * prev_total_width)\n",
    "                sys.stdout.write('\\r')\n",
    "            else:\n",
    "                sys.stdout.write('\\n')\n",
    "\n",
    "            if self.target is not None:\n",
    "                numdigits = int(np.floor(np.log10(self.target))) + 1\n",
    "                barstr = '%%%dd/%d [' % (numdigits, self.target)\n",
    "                bar = barstr % current\n",
    "                prog = float(current) / self.target\n",
    "                prog_width = int(self.width * prog)\n",
    "                if prog_width > 0:\n",
    "                    bar += ('=' * (prog_width - 1))\n",
    "                    if current < self.target:\n",
    "                        bar += '>'\n",
    "                    else:\n",
    "                        bar += '='\n",
    "                bar += ('.' * (self.width - prog_width))\n",
    "                bar += ']'\n",
    "            else:\n",
    "                bar = '%7d/Unknown' % current\n",
    "\n",
    "            self._total_width = len(bar)\n",
    "#             sys.stdout.write(bar)\n",
    "            print(bar)\n",
    "\n",
    "            if current:\n",
    "                time_per_unit = (now - self._start) / current\n",
    "            else:\n",
    "                time_per_unit = 0\n",
    "            if self.target is not None and current < self.target:\n",
    "                eta = time_per_unit * (self.target - current)\n",
    "                if eta > 3600:\n",
    "                    eta_format = '%d:%02d:%02d' % (eta // 3600,\n",
    "                                                   (eta % 3600) // 60,\n",
    "                                                   eta % 60)\n",
    "                elif eta > 60:\n",
    "                    eta_format = '%d:%02d' % (eta // 60, eta % 60)\n",
    "                else:\n",
    "                    eta_format = '%ds' % eta\n",
    "\n",
    "                info = ' - ETA: %s' % eta_format\n",
    "            else:\n",
    "                if time_per_unit >= 1:\n",
    "                    info += ' %.0fs/step' % time_per_unit\n",
    "                elif time_per_unit >= 1e-3:\n",
    "                    info += ' %.0fms/step' % (time_per_unit * 1e3)\n",
    "                else:\n",
    "                    info += ' %.0fus/step' % (time_per_unit * 1e6)\n",
    "\n",
    "            for k in self._values_order:\n",
    "                info += ' - %s:' % k\n",
    "                if isinstance(self._values[k], list):\n",
    "                    avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n",
    "                    if abs(avg) > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                else:\n",
    "                    info += ' %s' % self._values[k]\n",
    "\n",
    "            self._total_width += len(info)\n",
    "            if prev_total_width > self._total_width:\n",
    "                info += (' ' * (prev_total_width - self._total_width))\n",
    "\n",
    "            if self.target is not None and current >= self.target:\n",
    "                info += '\\n'\n",
    "\n",
    "#             sys.stdout.write(info)\n",
    "            print(info)\n",
    "\n",
    "        elif self.verbose == 2:\n",
    "            if self.target is None or current >= self.target:\n",
    "                for k in self._values_order:\n",
    "                    info += ' - %s:' % k\n",
    "                    avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n",
    "                    if avg > 1e-3:\n",
    "                        info += ' %.4f' % avg\n",
    "                    else:\n",
    "                        info += ' %.4e' % avg\n",
    "                info += '\\n'\n",
    "\n",
    "                sys.stdout.write(info)\n",
    "                print(info)\n",
    "        self._last_update = now\n",
    "\n",
    "    def add(self, n, values=None):\n",
    "        self.update(self._seen_so_far + n, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b84056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:51.415158Z",
     "iopub.status.busy": "2023-04-10T08:55:51.414892Z",
     "iopub.status.idle": "2023-04-10T08:55:57.972237Z",
     "shell.execute_reply": "2023-04-10T08:55:57.971126Z"
    },
    "papermill": {
     "duration": 6.565011,
     "end_time": "2023-04-10T08:55:57.974841",
     "exception": false,
     "start_time": "2023-04-10T08:55:51.409830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import tensorflow as tf\n",
    "# from .dataset import Dataset\n",
    "# from .models import EdgeModel, InpaintingModel\n",
    "# from .utils import Progbar, create_dir, stitch_images, imsave\n",
    "# from .metrics import PSNR, EdgeAccuracy\n",
    "\n",
    "\n",
    "class EdgeConnect():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(str(e) for e in config.GPU)\n",
    "        if torch.cuda.is_available():\n",
    "            config.DEVICE = torch.device(\"cuda\")\n",
    "            torch.backends.cudnn.benchmark = True   # cudnn auto-tuner\n",
    "        else:\n",
    "            config.DEVICE = torch.device(\"cpu\")\n",
    "        \n",
    "        if config.MODEL == 1:\n",
    "            model_name = 'edge'\n",
    "        elif config.MODEL == 2:\n",
    "            model_name = 'inpaint'\n",
    "        elif config.MODEL == 3:\n",
    "            model_name = 'edge_inpaint'\n",
    "        elif config.MODEL == 4:\n",
    "            model_name = 'joint'\n",
    "        self.debug = True\n",
    "        self.model_name = model_name\n",
    "        # detect and init the TPU\n",
    "#         tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "#         # instantiate a distribution strategy\n",
    "#         tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "#         # instantiating the model in the strategy scope creates the model on the TPU\n",
    "#         with tpu_strategy.scope():\n",
    "        self.edge_model = EdgeModel(config).to(config.DEVICE)\n",
    "        self.inpaint_model = InpaintingModel(config).to(config.DEVICE)\n",
    "        \n",
    "        self.edgeacc = EdgeAccuracy(config.EDGE_THRESHOLD).to(config.DEVICE)\n",
    "        self.psnr = PSNR(255.0).to(config.DEVICE)\n",
    "        if self.config.MODE == 2:\n",
    "            self.test_dataset = Dataset(256, '/kaggle/input/train-test-set/val_256/', augment=False, training=True, isVal=True)\n",
    "        else:\n",
    "            self.train_dataset = Dataset(256, '/kaggle/input/places365/data_256/', augment=True, training=True, isVal=False)\n",
    "            self.val_dataset = Dataset(256, '/kaggle/input/train-test-set/val_256/', augment=False, training=True, isVal=True)\n",
    "            self.sample_iterator = self.val_dataset.create_iterator(config.SAMPLE_SIZE)\n",
    "        \n",
    "        self.samples_path = os.path.join(config.PATH, 'samples')\n",
    "        self.results_path = os.path.join(config.PATH, 'results')\n",
    "        \n",
    "        self.log_file = os.path.join(config.PATH, 'log_' + model_name + '.dat')\n",
    "        \n",
    "    def train(self):\n",
    "        # Train\n",
    "        epoch = 0\n",
    "        keep_training = True\n",
    "        model = self.config.MODEL\n",
    "        max_iteration = int(float((self.config.MAX_ITERS)))\n",
    "        \n",
    "        total = len(self.train_dataset)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            num_workers=4,\n",
    "            drop_last=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        self.load()\n",
    "        while(keep_training):\n",
    "            epoch += 1\n",
    "            print('\\n\\nTraining epoch: %d' % epoch)\n",
    "\n",
    "            progbar = Progbar(total, width=20, verbose=1, stateful_metrics=['epoch', 'iter'])\n",
    "            for items in train_loader:\n",
    "\n",
    "                self.edge_model.train()\n",
    "                self.inpaint_model.train()\n",
    "                images, images_gray, edges, masks = self.cuda(*items)\n",
    "                # train\n",
    "                outputs, gen_loss, dis_loss, logs = self.edge_model.process(images_gray, edges, masks)\n",
    "\n",
    "                # metrics\n",
    "                precision, recall = self.edgeacc(edges * masks, outputs * masks)\n",
    "                logs.append(('precision', precision.item()))\n",
    "                logs.append(('recall', recall.item()))\n",
    "\n",
    "                # backward\n",
    "                self.edge_model.backward(gen_loss, dis_loss)\n",
    "\n",
    "                outputs, gen_loss, dis_loss, logsInpaint = self.inpaint_model.process(images, edges, masks)\n",
    "                logs += logsInpaint\n",
    "                outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "                # metrics\n",
    "                psnr = self.psnr(self.postprocess(images), self.postprocess(outputs_merged))\n",
    "                mae = (torch.sum(torch.abs(images - outputs_merged)) / torch.sum(images)).float()\n",
    "                logs.append(('psnr', psnr.item()))\n",
    "                logs.append(('mae', mae.item()))\n",
    "\n",
    "                # backward\n",
    "                self.inpaint_model.backward(gen_loss, dis_loss)\n",
    "                iteration = self.inpaint_model.iteration\n",
    "\n",
    "                if iteration >= max_iteration:\n",
    "                    keep_training = False\n",
    "                    break\n",
    "\n",
    "                logs = [\n",
    "                    (\"epoch\", epoch),\n",
    "                    (\"iter\", iteration),\n",
    "                ] + logs\n",
    "\n",
    "                progbar.add(len(images), values=logs if self.config.VERBOSE else [x for x in logs if not x[0].startswith('l_')])\n",
    "\n",
    "                # log model at checkpoints\n",
    "                if self.config.LOG_INTERVAL and iteration % self.config.LOG_INTERVAL == 0:\n",
    "                    self.log(logs)\n",
    "\n",
    "                # sample model at checkpoints\n",
    "                if self.config.SAMPLE_INTERVAL and iteration % self.config.SAMPLE_INTERVAL == 0:\n",
    "                    self.sample()\n",
    "\n",
    "                # evaluate model at checkpoints\n",
    "#                 if self.config.EVAL_INTERVAL and iteration % self.config.EVAL_INTERVAL == 0:\n",
    "#                     print('\\nstart eval...\\n')\n",
    "#                     self.eval()\n",
    "\n",
    "                # save model at checkpoints\n",
    "                if self.config.SAVE_INTERVAL and iteration % self.config.SAVE_INTERVAL == 0:\n",
    "                    self.save()\n",
    "\n",
    "            print('\\nEnd training....')\n",
    "    \n",
    "    def cuda(self, *args):\n",
    "        return (item.to(self.config.DEVICE) for item in args)\n",
    "    \n",
    "    def load(self):\n",
    "        if self.config.MODEL == 1:\n",
    "            self.edge_model.load()\n",
    "\n",
    "        elif self.config.MODEL == 2:\n",
    "            self.inpaint_model.load()\n",
    "\n",
    "        else:\n",
    "            self.edge_model.load()\n",
    "            self.inpaint_model.load()\n",
    "\n",
    "    def save(self):\n",
    "        if self.config.MODEL == 1:\n",
    "            self.edge_model.save()\n",
    "\n",
    "        elif self.config.MODEL == 2 or self.config.MODEL == 3:\n",
    "            self.inpaint_model.save()\n",
    "\n",
    "        else:\n",
    "            self.edge_model.save()\n",
    "            self.inpaint_model.save()\n",
    "        \n",
    "    def log(self, logs):\n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write('%s\\n' % ' '.join([str(item[1]) for item in logs]))\n",
    "            \n",
    "    def postprocess(self, img):\n",
    "        # [0, 1] => [0, 255]\n",
    "        img = img * 255.0\n",
    "        img = img.permute(0, 2, 3, 1)\n",
    "        return img.int()\n",
    "    \n",
    "    def eval(self):\n",
    "        val_loader = DataLoader(\n",
    "            dataset=self.val_dataset,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            drop_last=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        model = self.config.MODEL\n",
    "        total = len(self.val_dataset)\n",
    "\n",
    "        self.edge_model.eval()\n",
    "#         self.inpaint_model.eval()\n",
    "\n",
    "        progbar = Progbar(total, width=20, stateful_metrics=['it'])\n",
    "        iteration = 0\n",
    "\n",
    "        for items in val_loader:\n",
    "            iteration += 1\n",
    "            images, images_gray, edges, masks = self.cuda(*items)\n",
    "\n",
    "            # edge model\n",
    "            if model == 1:\n",
    "                # eval\n",
    "                outputs, gen_loss, dis_loss, logs = self.edge_model.process(images_gray, edges, masks)\n",
    "\n",
    "                # metrics\n",
    "                precision, recall = self.edgeacc(edges * masks, outputs * masks)\n",
    "                logs.append(('precision', precision.item()))\n",
    "                logs.append(('recall', recall.item()))\n",
    "        logs = [(\"it\", iteration), ] + logs\n",
    "        progbar.add(len(images), values=logs)\n",
    "        \n",
    "    def test(self):\n",
    "        self.edge_model.eval()\n",
    "        self.inpaint_model.eval()\n",
    "\n",
    "        model = self.config.MODEL\n",
    "        create_dir(self.results_path)\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=self.test_dataset,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "        index = 0\n",
    "        for items in test_loader:\n",
    "            name = self.test_dataset.load_name(index)\n",
    "            images, images_gray, edges, masks = self.cuda(*items)\n",
    "            index += 1\n",
    "\n",
    "            # edge model\n",
    "            if model == 1:\n",
    "                outputs = self.edge_model(images_gray, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (edges * (1 - masks))\n",
    "\n",
    "            # inpaint model\n",
    "            elif model == 2:\n",
    "                outputs = self.inpaint_model(images, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "            # inpaint with edge model / joint model\n",
    "            else:\n",
    "                print('in joint')\n",
    "                inputs = (images * (1 - masks)) + masks\n",
    "                edges = self.edge_model(images_gray, edges, masks).detach()\n",
    "                outputs = self.inpaint_model(images, edges, masks)\n",
    "                outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "            output = self.postprocess(outputs_merged)[0]\n",
    "            \n",
    "            images = stitch_images(\n",
    "            self.postprocess(images),\n",
    "            self.postprocess(inputs),\n",
    "            self.postprocess(edges),\n",
    "            self.postprocess(outputs_merged)\n",
    "        )\n",
    "            \n",
    "            path = os.path.join(self.results_path, name)\n",
    "            print(index, name)\n",
    "            images.save(path)\n",
    "#             imsave(output, path)\n",
    "\n",
    "#             if self.debug:\n",
    "#                 edges = self.postprocess(1 - edges)[0]\n",
    "#                 masked = self.postprocess(images * (1 - masks) + masks)[0]\n",
    "#                 fname, fext = name.split('.')\n",
    "\n",
    "#                 imsave(edges, os.path.join(self.results_path, fname + '_edge.' + fext))\n",
    "#                 imsave(masked, os.path.join(self.results_path, fname + '_masked.' + fext))\n",
    "\n",
    "        print('\\nEnd test....')     \n",
    "        \n",
    "    def sample(self, it=None):\n",
    "        # do not sample when validation set is empty\n",
    "        if len(self.val_dataset) == 0:\n",
    "            return\n",
    "\n",
    "        self.edge_model.eval()\n",
    "        self.inpaint_model.eval()\n",
    "\n",
    "        model = self.config.MODEL\n",
    "        items = next(self.sample_iterator)\n",
    "        images, images_gray, edges, masks = self.cuda(*items)\n",
    "\n",
    "        # edge model\n",
    "        if model == 1:\n",
    "            iteration = self.edge_model.iteration\n",
    "            inputs = (images_gray * (1 - masks)) + masks\n",
    "            outputs = self.edge_model(images_gray, edges, masks)\n",
    "            outputs_merged = (outputs * masks) + (edges * (1 - masks))\n",
    "\n",
    "        # inpaint model\n",
    "        elif model == 2:\n",
    "            iteration = self.inpaint_model.iteration\n",
    "            inputs = (images * (1 - masks)) + masks\n",
    "            outputs = self.inpaint_model(images, edges, masks)\n",
    "            outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "        # inpaint with edge model / joint model\n",
    "        else:\n",
    "            iteration = self.inpaint_model.iteration\n",
    "            inputs = (images * (1 - masks)) + masks\n",
    "            outputs = self.edge_model(images_gray, edges, masks).detach()\n",
    "            edges = (outputs * masks + edges * (1 - masks)).detach()\n",
    "            outputs = self.inpaint_model(images, edges, masks)\n",
    "            outputs_merged = (outputs * masks) + (images * (1 - masks))\n",
    "\n",
    "        if it is not None:\n",
    "            iteration = it\n",
    "\n",
    "        image_per_row = 2\n",
    "        if self.config.SAMPLE_SIZE <= 6:\n",
    "            image_per_row = 1\n",
    "\n",
    "        images = stitch_images(\n",
    "            self.postprocess(images),\n",
    "            self.postprocess(inputs),\n",
    "            self.postprocess(edges),\n",
    "            self.postprocess(outputs),\n",
    "            self.postprocess(outputs_merged),\n",
    "            img_per_row = image_per_row\n",
    "        )\n",
    "\n",
    "\n",
    "        path = os.path.join(self.samples_path, self.model_name)\n",
    "        name = os.path.join(path, str(iteration).zfill(5) + \".png\")\n",
    "        create_dir(path)\n",
    "        print('\\nsaving sample ' + name)\n",
    "        images.save(name)\n",
    "\n",
    "    #             images = edges[0,...]\n",
    "    #             images = images[0,...]\n",
    "    #             print(tf.shape(images))\n",
    "    #             arr_ = np.squeeze(images) # you can give axis attribute if you wanna squeeze in specific dimension\n",
    "    #             plt.imshow(arr_, cmap=\"gray\")\n",
    "    #             plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cc71e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-10T08:55:57.986438Z",
     "iopub.status.busy": "2023-04-10T08:55:57.984344Z",
     "iopub.status.idle": "2023-04-10T08:56:18.059183Z",
     "shell.execute_reply": "2023-04-10T08:56:18.058126Z"
    },
    "papermill": {
     "duration": 20.082839,
     "end_time": "2023-04-10T08:56:18.061935",
     "exception": false,
     "start_time": "2023-04-10T08:55:57.979096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e7875c2b22405387977880354b05d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25049\n",
      "Loading EdgeModel generator...\n",
      "Loading EdgeModel discriminator...\n",
      "Loading InpaintingModel generator...\n",
      "Loading InpaintingModel discriminator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:103: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "saving sample /kaggle/working/samples/joint/2000000.png\n"
     ]
    }
   ],
   "source": [
    "config = Config()            \n",
    "edgeConnect = EdgeConnect(config)\n",
    "edgeConnect.load()\n",
    "edgeConnect.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.742807,
   "end_time": "2023-04-10T08:56:21.406271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-10T08:55:38.663464",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "030b9968239c4796a79b371776126329": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0c989691a97f4a63896145506d45973c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_030b9968239c4796a79b371776126329",
       "max": 574673361.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9f534b2b4e96478285182b5f208e9c15",
       "value": 574673361.0
      }
     },
     "0d59ecad7bad468fb2c6c6bad5696311": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6b008a8bf0414c3a96b3171a91fdcb23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d0564bd49b974b14881e67bd769fd8b1",
       "placeholder": "​",
       "style": "IPY_MODEL_79e3801092fd43c9831eb14cb73d1abd",
       "value": "100%"
      }
     },
     "79e3801092fd43c9831eb14cb73d1abd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "81967bb1693442e981ec08e581063045": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99e7875c2b22405387977880354b05d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6b008a8bf0414c3a96b3171a91fdcb23",
        "IPY_MODEL_0c989691a97f4a63896145506d45973c",
        "IPY_MODEL_c73e131063634c33be2c2956ffc3190b"
       ],
       "layout": "IPY_MODEL_c61904c1dea64d5d8ed5479997ec8aa0"
      }
     },
     "9f534b2b4e96478285182b5f208e9c15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c61904c1dea64d5d8ed5479997ec8aa0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c73e131063634c33be2c2956ffc3190b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_81967bb1693442e981ec08e581063045",
       "placeholder": "​",
       "style": "IPY_MODEL_0d59ecad7bad468fb2c6c6bad5696311",
       "value": " 548M/548M [00:02&lt;00:00, 258MB/s]"
      }
     },
     "d0564bd49b974b14881e67bd769fd8b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
